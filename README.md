# E-Commerce-Analytics
E-Commerce Data Warehouse Pipeline
### **Overview**
This project builds a robust end-to-end Extract, Transform, Load (ETL) pipeline for e-commerce analytics using the Olist dataset. The pipeline is designed to analyze key performance indicators (KPIs) in an e-commerce environment. It follows best practices in data engineering, featuring a modular architecture, automated transformation logic, workflow orchestration, and containerization.

### **Project Goals**
- Ingest raw e-commerce datasets.
- Transform and model the data using dbt (Data Build Tool).
- Orchestrate the ETL process using Apache Airflow.
- Containerize the environment using Docker and Docker Compose.
- Store the transformed data in a PostgreSQL database.
- Provide insights into e-commerce KPIs through analytics.

### **Tools Used**
- **dbt**: For data transformation and modeling.
- **Apache Airflow**: For workflow orchestration.
- **Docker & Docker Compose**: For environment containerization.
- **PostgreSQL**: As the target data warehouse.
- **Python (Pandas)**: For data ingestion and preprocessing.

---

## ðŸ“‚ Project Structure

The project is organized into several directories, each serving a specific purpose. Below is a detailed breakdown of the project structure:

### **1. `airflow` Directory**
This directory contains the Apache Airflow configuration and DAGs (Directed Acyclic Graphs) for orchestrating the ETL pipeline.

#### Subdirectories:
- **`dags`**: Contains Python scripts defining the DAGs for the ETL process.
  - `ecommerce_pipeline.py`: Main DAG for the e-commerce data pipeline.
  - `test_dag.py`: Example DAG for testing purposes.
- **`logs`**: Stores logs generated by Airflow during execution.
- **`plugins`**: Custom plugins for Airflow (if any).

### **2. `data` Directory**
This directory holds the raw datasets from the Olist dataset.

#### Files:
- `olist_customers_dataset.csv`
- `olist_geolocation_dataset.csv`
- `olist_order_items_dataset.csv`
- `olist_order_payments_dataset.csv`
- `olist_order_reviews_dataset.csv`
- `olist_orders_dataset.csv`
- `olist_products_dataset.csv`
- `olist_sellers_dataset.csv`
- `product_category_name_translation.csv`

### **3. `dbt-env` Directory**
This directory contains the virtual environment for dbt.

#### Subdirectories:
- **`Include`**: Standard include files for the virtual environment.
- **`Lib`**: Python libraries installed in the virtual environment.
- **`Scripts`**: Scripts related to the virtual environment.

### **4. `dbt-profiles` Directory**
This directory stores configuration files for dbt.

#### Files:
- `profiles.yml`: Configuration file for dbt, specifying database credentials and profiles.
- `pyenv.cfg`: Configuration file for the Python environment.

### **5. `ecommerce_dbt` Directory**
This is the main directory for the dbt project, containing all the dbt-related files and configurations.

#### Subdirectories:
- **`analyses`**: SQL queries for generating analytical reports.
- **`dbt_packages`**: External dbt packages (if any).
- **`logs`**: Logs generated by dbt during execution.
- **`macros`**: Custom SQL macros for reuse across models.
- **`models`**: SQL models for transforming and modeling the data.
- **`seeds`**: Seed files for loading static data into the data warehouse.
- **`snapshots`**: Snapshot models for capturing point-in-time data.
- **`tests`**: Tests for validating the data quality and integrity.

#### Files:
- `.gitignore`: Specifies files and directories to ignore in version control.
- `.dbt_project.yml`: Configuration file for the dbt project.
- `README.md`: Documentation for the dbt project.
- `init.sql`: SQL script for initializing the database schema.
- `load_all_datasets.py`: Python script for loading raw datasets into the database.
- `.env`: Environment variables for the dbt project.
- `.gitignore`: Specifies files and directories to ignore in version control.
- `docker-compose.yml`: Configuration file for Docker Compose.
- `Dockerfile`: Dockerfile for building the containerized environment.
- `requirements.txt`: List of Python dependencies required for the project.

### **6. Other Files**
- **`.user.yml`**: User-specific configuration file for dbt.
- **`first10rows_from_each_table.py`**: Python script for inspecting the first 10 rows of each table.
- **`jdtb`**: Likely a placeholder or typo; ensure this is correctly named if it serves a specific purpose.

---

## ðŸ“„ License

This project is licensed under the MIT License. See the `LICENSE` file for more details.

---

